{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play around CCA using sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import nilearn as nil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import csv\n",
    "import sklearn\n",
    "from sklearn.cross_decomposition import CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.22'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get behav data from Marco\n",
    "Sub = np.loadtxt('/data/backed_up/shared/mpipoly_HCPtest/batch_data/subjects.txt', dtype=str)\n",
    "Task1=['WM', 'SOCIAL', 'RELATIONAL', 'GAMBLING', 'EMOTION', 'MOTOR', 'LANGUAGE','rfMRI','task']\n",
    "data_dir = '/data/backed_up/shared/mpipoly_HCPtest/batch_data/'\n",
    "\n",
    "csv2 = pd.read_csv('/data/backed_up/shared/mpipoly_HCPtest/batch_data/PYTHON_sTATS/unrestricted_mpipoly_11_19_2019_12_1_22.csv') \n",
    "df2=csv2[csv2['Subject'].isin(list(Sub))] \n",
    "\n",
    "newDF2=df2[['Emotion_Task_Acc','Emotion_Task_Median_RT','Language_Task_Acc','Language_Task_Median_RT','Gambling_Task_Median_RT_Larger','Gambling_Task_Median_RT_Smaller','Relational_Task_Acc','Relational_Task_Median_RT','WM_Task_Acc','WM_Task_Median_RT','Social_Task_Median_RT_TOM','Social_Task_Median_RT_Random','Social_Task_Median_RT_Unsure']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0261526   0.05646732  0.02090673 ... -0.01015351 -0.06936608\n",
      "   0.1371647 ]\n",
      " [-0.00525365  0.07348273  0.01819496 ...  0.15467393 -0.05287683\n",
      "   0.0529021 ]\n",
      " [ 0.08448181 -0.00367428 -0.03634613 ... -0.027576    0.04881956\n",
      "   0.02245879]\n",
      " ...\n",
      " [-0.01173314  0.12081812 -0.0515199  ... -0.00310758  0.04449047\n",
      "  -0.06093103]\n",
      " [-0.0188491   0.08961105 -0.00266385 ... -0.00866891 -0.04057553\n",
      "   0.05887987]\n",
      " [ 0.01949048  0.00064084 -0.09660682 ... -0.08229227 -0.02524862\n",
      "  -0.01831261]]\n",
      "100\n",
      "dict_keys(['Gordon_Variate_X', 'Glasser_Variate_X'])\n"
     ]
    }
   ],
   "source": [
    "#get FC matrices\n",
    "task_keys={}\n",
    "#Glasser_Variate_X=np.array([])\n",
    "#Gordon_Variate_X=np.array([])\n",
    "final={}\n",
    "for t in Task1:\n",
    "    task_keys[t]={}\n",
    "    ls_Glass=list()\n",
    "    ls_Gord=list()\n",
    "    final[t]={}\n",
    "    for s in Sub:\n",
    "        task_keys[t][s]={}\n",
    "        task_keys[t][s]['Thal_X_Gord'] = (data_dir+'PYTHON_sTATS'+'/'+'{0}'+'/'+'{1}'+'/'+'{0}'+'_'+'{1}'+'_'+'Thal_X_Gord'+'.npy').format(s,t)\n",
    "        #task_keys[s][t]['Thal_X_Gord'] = task_level[s]['rfMRI'].update({'Thal_X_Gord': task_level['GordParc'].get_fdata()})\n",
    "        task_keys[t][s]['Thal_X_Gord'] = np.load(task_keys[t][s]['Thal_X_Gord'])\n",
    "        task_keys[t][s]['Thal_X_Glass'] = (data_dir+'PYTHON_sTATS'+'/'+'{0}'+'/'+'{1}'+'/'+'{0}'+'_'+'{1}'+'_'+'Thal_X_Glass'+'.npy').format(s,t)\n",
    "        task_keys[t][s]['Thal_X_Glass'] = np.load(task_keys[t][s]['Thal_X_Glass'])\n",
    "        task_keys[t][s]['Thal_X_Gord'] = task_keys[t][s]['Thal_X_Gord'].flatten()\n",
    "        task_keys[t][s]['Thal_X_Glass'] = task_keys[t][s]['Thal_X_Glass'].flatten()\n",
    "        ls_Glass.append(np.array([task_keys[t][s]['Thal_X_Glass']]))\n",
    "        ls_Gord.append(np.array(task_keys[t][s]['Thal_X_Gord']))\n",
    "        #Glasser_Variate_X=np.vstack((Glasser_Variate_X,Tmp_Glass))\n",
    "        #Glasser_Variate_X=np.append(Glasser_Variate_X, task_keys[t][s]['Thal_X_Glass'])\n",
    "        #Gordon_Variate_X=np.append(Gordon_Variate_X, task_keys[t][s]['Thal_X_Gord'])\n",
    "    final[t]={'Gordon_Variate_X': np.vstack(ls_Gord)}\n",
    "    final[t].update({'Glasser_Variate_X': np.vstack(ls_Gord)})\n",
    "    #print(final['WM']['Glasser_Variate_X'])\n",
    "    #/data/backed_up/shared/mpipoly_HCPtest/batch_data/PYTHON_sTATS/task/CCA_Analyses\n",
    "    np.save(f'/data/backed_up/shared/mpipoly_HCPtest/batch_data/PYTHON_sTATS/task/CCA_Analyses/{t}_ALL_cca_Thal_X_Gord', final[t]['Gordon_Variate_X'])\n",
    "    np.save(f'/data/backed_up/shared/mpipoly_HCPtest/batch_data/PYTHON_sTATS/task/CCA_Analyses/{t}_ALL_cca_Thal_X_Glass', final[t]['Glasser_Variate_X'])\n",
    "    \n",
    "print(final['WM']['Glasser_Variate_X']) # Check Stacking\n",
    "print(len(final['WM']['Glasser_Variate_X'])) # Check 100 flattened arrays are present\n",
    "print(final['WM'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 844488)\n",
      "[[ 0.0261526   0.05646732  0.02090673 ... -0.01015351 -0.06936608\n",
      "   0.1371647 ]\n",
      " [-0.00525365  0.07348273  0.01819496 ...  0.15467393 -0.05287683\n",
      "   0.0529021 ]\n",
      " [ 0.08448181 -0.00367428 -0.03634613 ... -0.027576    0.04881956\n",
      "   0.02245879]\n",
      " ...\n",
      " [-0.01173314  0.12081812 -0.0515199  ... -0.00310758  0.04449047\n",
      "  -0.06093103]\n",
      " [-0.0188491   0.08961105 -0.00266385 ... -0.00866891 -0.04057553\n",
      "   0.05887987]\n",
      " [ 0.01949048  0.00064084 -0.09660682 ... -0.08229227 -0.02524862\n",
      "  -0.01831261]]\n"
     ]
    }
   ],
   "source": [
    "X = final['WM']['Glasser_Variate_X']\n",
    "print(X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Acc=newDF2['WM_Task_Acc']\n",
    "RT=newDF2['WM_Task_Median_RT']\n",
    "Y = np.squeeze(np.dstack((Acc,RT)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00  1.45738866e-17  1.00000000e+00 -4.45190550e-17]\n",
      " [ 1.45738866e-17  1.00000000e+00 -1.22807133e-16  1.00000000e+00]\n",
      " [ 1.00000000e+00 -1.22807133e-16  1.00000000e+00 -4.77348547e-17]\n",
      " [-4.45190550e-17  1.00000000e+00 -4.77348547e-17  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#try CCA, and print correlation. These correlations are called the Canonical correlations\n",
    "cca1 = CCA(n_components=2)\n",
    "cca1.fit(X, Y)\n",
    "X_c, Y_c = cca1.fit_transform(X,Y)\n",
    "\n",
    "print(np.corrcoef(cca1.x_scores_.T, cca1.y_scores_.T))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(844488, 2)\n",
      "(844488, 2)\n",
      "(844488, 2)\n"
     ]
    }
   ],
   "source": [
    "# weights are the A and B in AX = BY # loadings and rotations are the PCA loadings which we will not use\n",
    "print(cca1.x_loadings_.shape)\n",
    "print(cca1.x_weights_.shape)\n",
    "print(cca1.x_rotations_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 844488)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cca1.x_scores_.shape\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the steve smith approach where we correlate scores with original FC\n",
    "r_vals = np.zeros(844488)\n",
    "\n",
    "for i in np.arange(844488):\n",
    "    r_vals[i]=np.corrcoef(cca1.x_scores_[:,0], X[:,i])[0,1]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.674299029493056e-05\n",
      "0.1030936836368332\n",
      "0.4865108385465558\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(r_vals))\n",
    "print(np.std(r_vals))\n",
    "print(np.max(r_vals))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CCA(copy=True, max_iter=500, n_components=2, scale=True, tol=1e-06)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let us compare the wieghts\n",
    "XWM = final['WM']['Glasser_Variate_X']\n",
    "AccWM=newDF2['WM_Task_Acc']\n",
    "RTWM=newDF2['WM_Task_Median_RT']\n",
    "Y = np.squeeze(np.dstack((Acc,RT)))\n",
    "cca1 = CCA(n_components=2)\n",
    "cca1.fit(XWM, Y)\n",
    "\n",
    "XL = final['SOCIAL']['Glasser_Variate_X']\n",
    "S1=newDF2['Social_Task_Median_RT_Unsure']\n",
    "S2=newDF2['Social_Task_Median_RT_TOM']\n",
    "YS = np.squeeze(np.dstack((S1,S2)))\n",
    "YS[np.isnan(YS)]=0\n",
    "cca2 = CCA(n_components=2)\n",
    "cca2.fit(XL, YS)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00, -1.38302098e-04, -4.12809404e-04,\n",
       "         7.52505226e-03],\n",
       "       [-1.38302098e-04,  1.00000000e+00,  3.99152540e-03,\n",
       "         1.13462001e-03],\n",
       "       [-4.12809404e-04,  3.99152540e-03,  1.00000000e+00,\n",
       "        -2.89557062e-04],\n",
       "       [ 7.52505226e-03,  1.13462001e-03, -2.89557062e-04,\n",
       "         1.00000000e+00]])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(cca1.x_weights_.T, cca2.x_weights_.T) # are they correlated at all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.multivariate.cancorr import CanCorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts=CanCorr(XWM, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
